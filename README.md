# Locable â€” AI-Powered Website Builder

Locable is an intelligent web development assistant that leverages local LLMs and vector retrieval to help you create professional websites. It combines a builder agent with Bootstrap-based templates and a Chroma vector database for intelligent context retrieval.

## Features

- **AI-Powered Code Generation**: Uses local Ollama models (qwen2.5-coder by default) to generate HTML, CSS, and JavaScript
- **Bootstrap Integration**: Locally-stored Bootstrap CSS and JavaScript files for consistent, responsive design
- **Vector Retrieval (RAG)**: Chroma-backed vector store indexes Bootstrap documentation for context-aware code generation
- **Interactive CLI**: Chat-based interface to request websites and iterate on designs
- **Tool Execution**: Write, read, and list files directly from the agent
- **Local-First**: All processing happens locally â€” no cloud dependencies or API calls

## Project Structure

```
locable/
â”œâ”€â”€ agent/                          # Agent orchestration
â”‚   â”œâ”€â”€ agent.py                    # Tool implementations (write_file, read_file, list_files)
â”‚   â”œâ”€â”€ builder_agent.py            # Main agent class with tool execution and model coordination
â”‚   â”œâ”€â”€ final_model.py              # FinalModelClient wrapper for Ollama API
â”‚   â””â”€â”€ tools.json                  # Tool definitions (schemas for LLM)
â”œâ”€â”€ rag/                            # Retrieval-Augmented Generation (RAG)
â”‚   â”œâ”€â”€ chroma_store.py             # ChromaVectorStore class for local vector persistence
â”‚   â”œâ”€â”€ embedding.py                # Embedding function using Ollama
â”‚   â”œâ”€â”€ vectorstore.py              # LocalVectorStore adapter (compatibility wrapper)
â”‚   â”œâ”€â”€ retriever.py                # Retriever class for document retrieval
â”‚   â””â”€â”€ __init__.py                 # Package exports
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bootstrap/                  # Bootstrap library files
â”‚   â”‚   â”œâ”€â”€ bootstrap.bundle.min.js # Bootstrap JavaScript (bundled Popper.js)
â”‚   â”‚   â””â”€â”€ bootstrap.min.css       # Bootstrap CSS
â”‚   â””â”€â”€ chroma/                     # Local vector store (auto-created)
â”‚       â”œâ”€â”€ documents.json          # Document chunks
â”‚       â”œâ”€â”€ ids.json                # Document IDs
â”‚       â”œâ”€â”€ metadatas.json          # Document metadata
â”‚       â””â”€â”€ embeddings.npy          # Vector embeddings (NumPy binary)
â”œâ”€â”€ site/                           # Generated website output
â”‚   â”œâ”€â”€ index.html                  # Main HTML file
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ bootstrap.min.css       # Copy of Bootstrap CSS
â”‚       â”œâ”€â”€ bootstrap.bundle.min.js # Copy of Bootstrap JS
â”‚       â”œâ”€â”€ styles.css              # Custom styles
â”‚       â””â”€â”€ script.js               # Custom JavaScript
â”œâ”€â”€ prompts/                        # LLM system prompts
â”‚   â””â”€â”€ system_prompt.txt           # Agent system instructions
â”œâ”€â”€ .gitignore                      # Git ignore rules
â”œâ”€â”€ LICENSE                         # MIT License
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # This file
```

## Installation

### Prerequisites
- Python 3.10+
- Ollama with qwen2.5-coder:14b-instruct model (or compatible LLM)
- 4GB+ RAM recommended
- Windows, macOS, or Linux

### Setup

1. **Navigate to the project:**
   ```bash
   cd locable
   ```

2. **Create and activate a virtual environment (recommended):**
   ```bash
   python -m venv venv
   # Windows
   venv\Scripts\activate
   # macOS/Linux
   source venv/bin/activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Ensure Ollama is running:**
   
   In one terminal, start the Ollama server:
   ```bash
   ollama serve
   ```
   
   In another terminal, pull the default model:
   ```bash
   ollama pull qwen2.5-coder:14b-instruct
   ```

## Quick Start

### Step 1: Index Bootstrap Files (One-Time Setup)

This step chunks and embeds your Bootstrap CSS/JS files into a local Chroma vector database:

```bash
python -m rag.chroma_store
```

Expected output:
```
Chroma Vector Store Indexer
Indexed 391 chunks into Chroma collection 'bootstrap' at 'data/chroma'.
Done. Indexed 391 chunks.
```

This creates vector files in `data/chroma/` and enables retrieval-augmented generation.

### Step 2: Run the Builder Agent

Start the interactive CLI:

```bash
python -m agent.builder_agent
```

You should see:
```
Builder Agent CLI
Type a request (ex: 'Create a simple landing page')
You: 
```

### Step 3: Describe Your Website

Type a natural language description of what you want:

```
You: create a comprehensive sushi website with a hero, menu with 6 animated cards, and contact section
```

The agent will:
1. Query the vector store for Bootstrap examples
2. Generate HTML, CSS, and JavaScript using the LLM
3. Execute the `write_file` tool to save files to `site/`
4. Display completion status

### Step 4: View Your Website

Open the generated website in a browser:

```bash
# Windows
start site\index.html

# macOS
open site/index.html

# Linux
xdg-open site/index.html
```

## How It Works

### Agent Loop

1. **User Input**: Describe what website you want
2. **Retrieval**: Agent queries Chroma vector store for relevant Bootstrap patterns
3. **Model Call**: LLM (Ollama) generates HTML/CSS/JS based on request + retrieved context
4. **Tool Execution**: Agent executes `write_file` to save generated code
5. **Iteration**: Refine by asking for changes

### Vector Store (RAG)

- **Chroma**: Local, persistent vector database
- **Embeddings**: Generated by Ollama's embedding model
- **Documents**: Bootstrap CSS/JS chunked into 1000-character pieces
- **Retrieval**: Top-3 relevant chunks injected into LLM context

### Tools Available to Agent

- `write_file(path, content)` â€” Write or create files
- `read_file(path)` â€” Read file contents
- `list_files()` â€” List files in the project

## Configuration

### Model Selection

Edit `agent/builder_agent.py` to use a different Ollama model:

```python
agent = BuilderAgent(
    model="llama2:13b",  # Change this
    host="http://localhost:11434"
)
```

Available models: `ollama list` or [ollama.ai/library](https://ollama.ai/library)

### Vector Store Settings

Edit `rag/chroma_store.py` to adjust chunking:

```python
indexed = store.index_bootstrap_files(
    source_dir="data/bootstrap",
    chunk_size=1000,      # Adjust chunk size
    overlap=200           # Adjust overlap
)
```

### System Prompt

Modify `prompts/system_prompt.txt` to change agent behavior and instructions.

## Troubleshooting

### Ollama Not Responding
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# If not, start it
ollama serve
```

### Chroma Vector Store Empty
```bash
# Re-index Bootstrap files
python -m rag.chroma_store
```

### Model Context Overflow (500 Error)
- Restart Ollama: `taskkill /F /IM ollama.exe && ollama serve`
- Use a smaller model: `ollama pull qwen2.5-coder:7b-instruct`
- Reduce retrieval size in `builder_agent.py` `_append_retrieval_context(k=2)`

### Slow Generation
- Use a smaller/faster model
- Reduce vector retrieval count
- Ensure sufficient RAM (8GB+ recommended for 14B models)

## Example Requests

### Simple Landing Page
```
You: create a simple landing page for a tech startup with navbar, hero section, features, and footer
```

### Product Showcase
```
You: build a product showcase website with header, 6 product cards with descriptions, and contact form
```

### Restaurant Website
```
You: create a restaurant website with hero, menu section with 8 items, photos, and contact info
```

Generated output appears at `site/index.html`.

## Development

### Adding New Tools

1. Implement in `agent/agent.py`
2. Add schema to `agent/tools.json`
3. Register in `BuilderAgent.execute_tool()`

### Testing

```bash
# Test vector store
python -c "from rag.chroma_store import ChromaVectorStore; s = ChromaVectorStore(); print(s.query('button styles', n_results=3))"

# Test agent
python -m agent.builder_agent
```

## Performance

- **Indexing**: ~30 seconds for Bootstrap files
- **Model Response**: 30-120 seconds (depends on model and prompt)
- **Vector Retrieval**: <100ms
- **File I/O**: <100ms

## Limitations

- Local LLMs may produce less refined code than GPT-4
- Vector retrieval returns Bootstrap patterns only
- No image generation (uses placeholder URLs)
- No database/backend generation
- Single-page sites only

## Future Enhancements

- [ ] Multi-page site generation
- [ ] Custom component libraries
- [ ] Real image generation
- [ ] Database schema generation
- [ ] API endpoint scaffolding
- [ ] Accessibility (a11y) checks
- [ ] Performance optimization
- [ ] Form validation generation

## License

MIT License â€” see `LICENSE` file for details.

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## Support

For issues or questions:
- Check Troubleshooting section above
- Review agent debug output (`[DEBUG]` logs)
- Ensure Ollama and dependencies are up-to-date
- Check that `data/chroma/` contains indexed files

## Acknowledgments

- **Bootstrap** â€” Frontend framework
- **Ollama** â€” Local LLM runtime
- **Chroma** â€” Vector database
- **Qwen** â€” Base LLM model

---

**Happy building! ðŸš€**
